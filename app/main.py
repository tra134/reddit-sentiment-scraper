# app/main.py - PHI√äN B·∫¢N ƒê√É S·ª¨A LOGIC HO√ÄN CH·ªàNH
import streamlit as st
import pandas as pd
import numpy as np
import requests
import re
from datetime import datetime, timedelta
import time
import sys
import os
import sqlite3
import hashlib
import feedparser
import json
import plotly.graph_objects as go
import plotly.express as px
import io
import base64
import tempfile

# --- TH√äM IMPORT CHO NLTK ---
import nltk
import random
import string
from threading import Lock

# C·∫•u h√¨nh NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    nltk.download('brown')
    nltk.download('punkt_tab')  # G√≥i m·ªõi cho NLTK b·∫£n g·∫ßn ƒë√¢y

from textblob import TextBlob
from collections import Counter

# --- 1. SETUP ENVIRONMENT ---
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import UI Module
try:
    from visualizations import ui
    UI_AVAILABLE = True
except ImportError:
    UI_AVAILABLE = False
    # T·∫°o UI functions c∆° b·∫£n n·∫øu kh√¥ng t√¨m th·∫•y module
    class SimpleUI:
        @staticmethod
        def load_css():
            st.markdown("""
            <style>
            .stButton>button { width: 100%; }
            .feature-card { padding: 1rem; border-radius: 10px; background: #f8f9fa; }
            .metric-card { padding: 1rem; border-radius: 10px; border: 1px solid #e0e0e0; }
            </style>
            """, unsafe_allow_html=True)
        
        @staticmethod
        def render_login_screen():
            st.markdown("# üîê Reddit Insider AI")
        
        @staticmethod
        def render_dashboard_header(username):
            st.markdown(f"# üëã Ch√†o {username}")
        
        @staticmethod
        def render_feature_card(icon, title, desc, btn_class, btn_text, callback):
            with st.container():
                st.markdown(f"### {icon} {title}")
                st.markdown(desc)
                if st.button(btn_text, key=btn_class):
                    callback()
        
        @staticmethod
        def render_history_list(history, delete_func):
            for item in history:
                col1, col2 = st.columns([4, 1])
                with col1:
                    st.markdown(f"**{item['title'][:50]}...**")
                    st.caption(f"üìÖ {item['timestamp']}")
                with col2:
                    if st.button("üóëÔ∏è", key=f"del_{item['id']}"):
                        delete_func(item['id'])
                        st.rerun()
        
        @staticmethod
        def render_trending_card(post, callback=None):
            """Hi·ªÉn th·ªã card b√†i vi·∫øt trending - FIXED VERSION"""
            with st.container():
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.markdown(f"### üìù {post.get('title', 'No Title')[:70]}...")
                    st.caption(f"r/{post.get('subreddit', 'unknown')} ‚Ä¢ üë§ {post.get('author', 'unknown')}")
                    st.caption(f"üëç {post.get('score', 0)} ‚Ä¢ üí¨ {post.get('comments_count', 0)} ‚Ä¢ üïê {post.get('time_str', '')}")
                
                with col2:
                    if st.button("üîç Ph√¢n t√≠ch", key=f"analyze_{post.get('id', '')}"):
                        if callback:
                            callback(post.get('url', ''))
                        else:
                            st.session_state.analyze_url = post.get('url', '')
                            st.session_state.auto_run = True
                            st.session_state.page = "Analysis"
                            st.rerun()
        
        @staticmethod
        def render_trend_analysis(analysis_result):
            """Hi·ªÉn th·ªã k·∫øt qu·∫£ ph√¢n t√≠ch trend v·ªõi ƒë·ªì th·ªã ƒë·∫ßy ƒë·ªß"""
            if not analysis_result:
                st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ph√¢n t√≠ch")
                return
            
            st.markdown("### üìà Ph√¢n t√≠ch xu h∆∞·ªõng chi ti·∫øt")
            
            # ========== PH·∫¶N 1: TH·ªêNG K√ä T·ªîNG QUAN ==========
            st.markdown("#### üìä Th·ªëng k√™ t·ªïng quan")
            summary = analysis_result.get('data_summary', {})
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìù B√†i vi·∫øt", summary.get('total_posts_analyzed', 0))
            with col2:
                st.metric("‚≠ê ƒêi·ªÉm TB", f"{summary.get('avg_score_per_post', 0):.1f}")
            with col3:
                st.metric("üí¨ B√¨nh lu·∫≠n TB", f"{summary.get('avg_comments_per_post', 0):.1f}")
            with col4:
                st.metric("üöÄ Engagement TB", f"{summary.get('avg_engagement_per_post', 0):.1f}")
            
            # ========== PH·∫¶N 2: ƒê·ªí TH·ªä D·ª∞ B√ÅO ==========
            forecast = analysis_result.get('forecast', {})
            if forecast and 'forecast' in forecast:
                forecast_data = forecast.get('forecast', [])
                if forecast_data:
                    st.markdown("#### üìà D·ª± b√°o Engagement")
                    
                    # T·∫°o DataFrame cho ƒë·ªì th·ªã
                    df_forecast = pd.DataFrame(forecast_data)
                    
                    # T·∫°o ƒë·ªì th·ªã v·ªõi Plotly
                    fig = go.Figure()
                    
                    # Th√™m ƒë∆∞·ªùng d·ª± b√°o ch√≠nh
                    fig.add_trace(go.Scatter(
                        x=df_forecast['date'],
                        y=df_forecast['predicted_engagement'],
                        mode='lines+markers',
                        name='D·ª± b√°o',
                        line=dict(color='#1f77b4', width=3),
                        marker=dict(size=8)
                    ))
                    
                    # Th√™m v√πng confidence interval
                    fig.add_trace(go.Scatter(
                        x=df_forecast['date'].tolist() + df_forecast['date'].tolist()[::-1],
                        y=df_forecast['predicted_upper'].tolist() + df_forecast['predicted_lower'].tolist()[::-1],
                        fill='toself',
                        fillcolor='rgba(31, 119, 180, 0.2)',
                        line=dict(color='rgba(255,255,255,0)'),
                        name='Kho·∫£ng tin c·∫≠y',
                        showlegend=True
                    ))
                    
                    # C·∫•u h√¨nh layout
                    fig.update_layout(
                        title='D·ª± b√°o Engagement 7 ng√†y t·ªõi',
                        xaxis_title='Ng√†y',
                        yaxis_title='Engagement',
                        hovermode='x unified',
                        template='plotly_white',
                        height=400
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Hi·ªÉn th·ªã xu h∆∞·ªõng
                    if 'trend_direction' in forecast:
                        trend_emoji = {
                            'TƒÉng m·∫°nh üöÄ': 'üöÄ',
                            'TƒÉng nh·∫π ‚ÜóÔ∏è': '‚ÜóÔ∏è',
                            'Gi·∫£m m·∫°nh üìâ': 'üìâ',
                            'Gi·∫£m nh·∫π ‚ÜòÔ∏è': '‚ÜòÔ∏è',
                            '·ªîn ƒë·ªãnh ‚û°Ô∏è': '‚û°Ô∏è',
                            'ƒêang ph√¢n t√≠ch üìä': 'üìä'
                        }.get(forecast['trend_direction'], 'üìä')
                        
                        st.info(f"**Xu h∆∞·ªõng:** {forecast['trend_direction']} {trend_emoji}")
            
            # ========== PH·∫¶N 3: ƒê·ªí TH·ªä GI·ªú CAO ƒêI·ªÇM ==========
            peak_hours = analysis_result.get('peak_hours', [])
            if peak_hours:
                st.markdown("#### üïí Gi·ªù cao ƒëi·ªÉm ƒëƒÉng b√†i")
                
                peak_df = pd.DataFrame(peak_hours)
                peak_df['hour_str'] = peak_df['hour'].apply(lambda x: f"{x:02d}:00")
                
                # S·∫Øp x·∫øp theo gi·ªù
                peak_df = peak_df.sort_values('hour')
                
                # T·∫°o ƒë·ªì th·ªã c·ªôt
                fig2 = px.bar(
                    peak_df,
                    x='hour_str',
                    y='total_engagement',
                    title='Engagement theo gi·ªù trong ng√†y',
                    labels={'hour_str': 'Gi·ªù', 'total_engagement': 'Engagement'},
                    color='total_engagement',
                    color_continuous_scale='Viridis'
                )
                
                fig2.update_layout(
                    xaxis_title='Gi·ªù',
                    yaxis_title='T·ªïng Engagement',
                    height=350,
                    template='plotly_white'
                )
                
                st.plotly_chart(fig2, use_container_width=True)
                
                # Hi·ªÉn th·ªã top 3 gi·ªù cao ƒëi·ªÉm
                if len(peak_hours) >= 3:
                    top_hours = sorted(peak_hours, key=lambda x: x['total_engagement'], reverse=True)[:3]
                    st.markdown("**‚è∞ Top 3 gi·ªù cao ƒëi·ªÉm:**")
                    for i, hour_data in enumerate(top_hours, 1):
                        hour = hour_data['hour']
                        engagement = hour_data['total_engagement']
                        posts = hour_data['post_count']
                        st.markdown(f"{i}. **{hour:02d}:00** - {engagement} engagement ({posts} b√†i)")
            
            # ========== PH·∫¶N 4: T·ª™ KH√ìA PH·ªî BI·∫æN ==========
            keywords = analysis_result.get('top_keywords', [])
            if keywords:
                st.markdown("#### üîë T·ª´ kh√≥a ph·ªï bi·∫øn")
                
                # T·∫°o word cloud ƒë∆°n gi·∫£n
                keywords_df = pd.DataFrame(keywords[:10])  # L·∫•y top 10
                
                if not keywords_df.empty:
                    # T·∫°o ƒë·ªì th·ªã thanh cho t·ª´ kh√≥a
                    fig3 = px.bar(
                        keywords_df,
                        x='keyword',
                        y='score',
                        title='Top t·ª´ kh√≥a',
                        labels={'keyword': 'T·ª´ kh√≥a', 'score': 'ƒê·ªô ph·ªï bi·∫øn'},
                        color='score',
                        color_continuous_scale='thermal'
                    )
                    
                    fig3.update_layout(
                        xaxis_title='T·ª´ kh√≥a',
                        yaxis_title='ƒê·ªô ph·ªï bi·∫øn',
                        height=350,
                        template='plotly_white',
                        xaxis_tickangle=-45
                    )
                    
                    st.plotly_chart(fig3, use_container_width=True)
                    
                    # Hi·ªÉn th·ªã danh s√°ch t·ª´ kh√≥a
                    keyword_list = " | ".join([f"**{k['keyword']}**" for k in keywords[:8]])
                    st.markdown(f"üìå *C√°c t·ª´ kh√≥a h√†ng ƒë·∫ßu:* {keyword_list}")
            
            # ========== PH·∫¶N 5: TH√îNG TIN B·ªî SUNG ==========
            st.markdown("---")
            col_info1, col_info2 = st.columns(2)
            
            with col_info1:
                if 'analysis_timestamp' in analysis_result:
                    ts = datetime.fromisoformat(analysis_result['analysis_timestamp'])
                    st.metric("üïê Th·ªùi gian ph√¢n t√≠ch", ts.strftime('%H:%M %d/%m/%Y'))
            
            with col_info2:
                if 'subreddit' in analysis_result:
                    st.metric("üë• C·ªông ƒë·ªìng", f"r/{analysis_result['subreddit']}")
            
            if 'note' in analysis_result:
                st.success(f"üí° {analysis_result['note']}")
        
        @staticmethod
        def render_sidebar_logged_in(username, groups, logout_callback, add_group_callback, delete_group_callback):
            with st.sidebar:
                st.markdown(f"### üë§ {username}")
                st.divider()
                
                # Navigation
                st.markdown("### üß≠ ƒêi·ªÅu h∆∞·ªõng")
                pages = {
                    "üìä Dashboard": "Dashboard",
                    "üìà Xu h∆∞·ªõng": "Trending",
                    "üîó Ph√¢n t√≠ch b√†i vi·∫øt": "Analysis"
                }
                
                for icon_name, page_name in pages.items():
                    if st.button(icon_name, use_container_width=True, key=f"nav_{page_name}"):
                        st.session_state.page = page_name
                        st.rerun()
                
                st.divider()
                
                # Groups management
                st.markdown("### üë• Nh√≥m theo d√µi")
                if not groups:
                    st.info("Ch∆∞a c√≥ nh√≥m n√†o")
                else:
                    for group in groups:
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"r/{group['subreddit']}")
                        with col2:
                            if st.button("üóëÔ∏è", key=f"del_group_{group['id']}"):
                                delete_group_callback(group['id'])
                                st.rerun()
                
                # Add group
                with st.form("add_group"):
                    new_group = st.text_input("Th√™m subreddit", placeholder="python")
                    if st.form_submit_button("‚ûï Th√™m", use_container_width=True):
                        if new_group:
                            add_group_callback(new_group)
                            st.rerun()
                
                st.divider()
                
                # Logout
                if st.button("üö™ ƒêƒÉng xu·∫•t", type="secondary", use_container_width=True):
                    logout_callback()
    
    ui = SimpleUI()

# Config API
try:
    GOOGLE_GEMINI_API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    GOOGLE_GEMINI_API_KEY = None

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# --- PAGE CONFIG (B·∫Øt bu·ªôc g·ªçi ƒë·∫ßu ti√™n) ---
st.set_page_config(
    page_title="Reddit Insider AI",
    page_icon="üíé",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==========================================
# 2. DATABASE MANAGER (SQLITE - LOCAL) v·ªõi thread safety
# ==========================================
db_lock = Lock()

class DBManager:
    def __init__(self, db_name="reddit_insider.db"):
        # S·ª≠ d·ª•ng temp directory cho Streamlit Cloud
        if os.environ.get('STREAMLIT_CLOUD'):
            temp_dir = tempfile.gettempdir()
            db_path = os.path.join(temp_dir, db_name)
        else:
            db_path = db_name
            
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self.create_tables()

    def create_tables(self):
        with db_lock:
            c = self.conn.cursor()
            c.execute('''CREATE TABLE IF NOT EXISTS users 
                         (id INTEGER PRIMARY KEY AUTOINCREMENT, username TEXT UNIQUE, password TEXT)''')
            c.execute('''CREATE TABLE IF NOT EXISTS groups 
                         (id INTEGER PRIMARY KEY AUTOINCREMENT, user_id INTEGER, subreddit TEXT)''')
            c.execute('''CREATE TABLE IF NOT EXISTS history 
                         (id INTEGER PRIMARY KEY AUTOINCREMENT, user_id INTEGER, title TEXT, url TEXT, timestamp TEXT)''')
            c.execute('''CREATE TABLE IF NOT EXISTS trend_cache 
                         (id INTEGER PRIMARY KEY AUTOINCREMENT, subreddit TEXT, data TEXT, 
                          last_updated TEXT, UNIQUE(subreddit))''')
            self.conn.commit()

    def register(self, username, password):
        with db_lock:
            c = self.conn.cursor()
            hashed = hashlib.sha256(password.encode()).hexdigest()
            try:
                c.execute("INSERT INTO users (username, password) VALUES (?, ?)", (username, hashed))
                self.conn.commit()
                return True
            except: 
                return False

    def login(self, username, password):
        with db_lock:
            c = self.conn.cursor()
            hashed = hashlib.sha256(password.encode()).hexdigest()
            c.execute("SELECT id, username FROM users WHERE username=? AND password=?", (username, hashed))
            return c.fetchone()

    def add_group(self, user_id, subreddit):
        with db_lock:
            c = self.conn.cursor()
            clean_sub = subreddit.replace('r/', '').replace('/', '').strip()
            if not clean_sub: 
                return False
            c.execute("SELECT id FROM groups WHERE user_id=? AND subreddit=?", (user_id, clean_sub))
            if not c.fetchone():
                c.execute("INSERT INTO groups (user_id, subreddit) VALUES (?, ?)", (user_id, clean_sub))
                self.conn.commit()
                return True
            return False

    def get_groups(self, user_id):
        with db_lock:
            c = self.conn.cursor()
            c.execute("SELECT id, subreddit FROM groups WHERE user_id=?", (user_id,))
            return [{'id': r[0], 'subreddit': r[1]} for r in c.fetchall()]

    def delete_group(self, group_id):
        with db_lock:
            c = self.conn.cursor()
            c.execute("DELETE FROM groups WHERE id=?", (group_id,))
            self.conn.commit()

    def add_history(self, user_id, title, url):
        with db_lock:
            c = self.conn.cursor()
            ts = datetime.now().strftime("%d/%m %H:%M")
            c.execute("SELECT id FROM history WHERE user_id=? AND url=? ORDER BY id DESC LIMIT 1", (user_id, url))
            if not c.fetchone():
                c.execute("INSERT INTO history (user_id, title, url, timestamp) VALUES (?, ?, ?, ?)", 
                          (user_id, title, url, ts))
                self.conn.commit()

    def get_history(self, user_id):
        with db_lock:
            c = self.conn.cursor()
            c.execute("SELECT id, title, url, timestamp FROM history WHERE user_id=? ORDER BY id DESC LIMIT 20", (user_id,))
            return [{'id': r[0], 'title': r[1], 'url': r[2], 'timestamp': r[3]} for r in c.fetchall()]

    def delete_history(self, hist_id):
        with db_lock:
            c = self.conn.cursor()
            c.execute("DELETE FROM history WHERE id=?", (hist_id,))
            self.conn.commit()

    def cache_trend_data(self, subreddit, data):
        """Cache k·∫øt qu·∫£ ph√¢n t√≠ch trend"""
        with db_lock:
            c = self.conn.cursor()
            ts = datetime.now().isoformat()
            
            try:
                def json_serializer(obj):
                    if isinstance(obj, (datetime, pd.Timestamp)):
                        return obj.isoformat()
                    elif isinstance(obj, (np.int64, np.float64)):
                        return int(obj) if isinstance(obj, np.int64) else float(obj)
                    elif hasattr(obj, '__dict__'):
                        return str(obj)
                    return str(obj)
                
                json_data = json.dumps(data, default=json_serializer, ensure_ascii=False)
                
                c.execute(
                    "INSERT OR REPLACE INTO trend_cache (subreddit, data, last_updated) VALUES (?, ?, ?)",
                    (subreddit, json_data, ts)
                )
                self.conn.commit()
                return True
                
            except Exception as e:
                print(f"‚ùå Cache error: {e}")
                return False

    def get_cached_trend_data(self, subreddit, max_age_minutes=30):
        """L·∫•y d·ªØ li·ªáu trend t·ª´ cache"""
        with db_lock:
            c = self.conn.cursor()
            cutoff_time = (datetime.now() - timedelta(minutes=max_age_minutes)).isoformat()
            
            c.execute(
                "SELECT data FROM trend_cache WHERE subreddit=? AND last_updated > ?",
                (subreddit, cutoff_time)
            )
            
            result = c.fetchone()
            if result:
                try:
                    loaded_data = json.loads(result[0], object_hook=self._json_date_hook)
                    return loaded_data
                except (json.JSONDecodeError, TypeError) as e:
                    print(f"‚ùå JSON decode error for cached data: {e}")
                    return None
            return None
    
    def _json_date_hook(self, dct):
        """H√†m helper ƒë·ªÉ chuy·ªÉn ƒë·ªïi chu·ªói ISO th√†nh datetime"""
        for key, value in dct.items():
            if isinstance(value, str):
                try:
                    dct[key] = datetime.fromisoformat(value)
                except (ValueError, TypeError):
                    pass
        return dct

db = DBManager()

# ==========================================
# 3. CORE LOGIC V·ªöI FALLBACK T·ª∞ ƒê·ªòNG - ƒê√É S·ª¨A
# ==========================================
class RedditLoader:
    def __init__(self):
        self.base_url = "https://www.reddit.com"
        # T·∫†O USER-AGENT NG·∫™U NHI√äN ƒê·ªÇ TR√ÅNH B·ªä CH·∫∂N
        random_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8))
        self.user_agent_base = f'web:reddit_insider_ai_{random_id}:v1.0.0'
        
        self.headers = {
            'User-Agent': self.user_agent_base,
            'Accept': 'application/json',
            'Accept-Language': 'en-US,en;q=0.9',
            'DNT': '1',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def fetch_data(self, url, retries=3, current_retry=0):
        """Fetch data v·ªõi fallback t·ª± ƒë·ªông ·∫©n"""
        if current_retry >= retries:
            return None, f"ƒê√£ th·ª≠ {retries} l·∫ßn nh∆∞ng kh√¥ng th√†nh c√¥ng"
        
        try:
            # Thay ƒë·ªïi User-Agent cho m·ªói l·∫ßn th·ª≠
            random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=5))
            self.session.headers['User-Agent'] = f'{self.user_agent_base}_{random_suffix}'
            
            if not url.startswith('http'):
                url = 'https://' + url
            
            if 'reddit.com' not in url:
                return None, "URL kh√¥ng ph·∫£i l√† Reddit"
            
            # FALLBACK T·ª∞ ƒê·ªòNG: Strategy chain
            if current_retry == 0:
                # Th·ª≠ JSON API ƒë·∫ßu ti√™n
                url = self._normalize_url(url)
            elif current_retry == 1:
                # Th·ª≠ old.reddit.com
                if 'www.reddit.com' in url:
                    url = url.replace('www.reddit.com', 'old.reddit.com')
                    url = self._normalize_url(url)
            elif current_retry == 2:
                # Th·ª≠ RSS feed v·ªõi format=xml
                url = self._convert_to_rss_url(url)
                if not url:
                    return None, "Kh√¥ng th·ªÉ chuy·ªÉn sang RSS"
            
            response = self.session.get(url, timeout=15, allow_redirects=True)
            
            if response.status_code == 200:
                if '.rss' in url or 'format=xml' in url:
                    # Parse RSS
                    return self._parse_rss_data(response.text, url)
                else:
                    try:
                        data = response.json()
                        return self._parse_reddit_data(data, url)
                    except json.JSONDecodeError:
                        return self._parse_html_fallback(response.text, url)
            
            elif response.status_code == 403:
                # T·ª± ƒë·ªông th·ª≠ ph∆∞∆°ng th·ª©c kh√°c
                time.sleep(1)
                return self.fetch_data(url, retries, current_retry + 1)
            
            elif response.status_code == 429:
                if current_retry < 2:
                    time.sleep(3)
                    return self.fetch_data(url, retries, current_retry + 1)
                else:
                    return None, "Reddit ƒëang ch·∫∑n y√™u c·∫ßu. Vui l√≤ng th·ª≠ l·∫°i sau 1 ph√∫t."
            
            elif response.status_code == 404:
                if current_retry < retries - 1:
                    return self.fetch_data(url, retries, current_retry + 1)
                return None, "Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt"
            
            else:
                return None, f"L·ªói HTTP {response.status_code}"
                
        except requests.exceptions.Timeout:
            return None, "Timeout khi k·∫øt n·ªëi ƒë·∫øn Reddit"
        except Exception as e:
            return None, f"L·ªói: {str(e)[:100]}"
    
    def _normalize_url(self, url):
        """Chu·∫©n h√≥a URL Reddit"""
        if '?' in url:
            url = url.split('?')[0]
        
        url = url.rstrip('/')
        
        if '/comments/' in url and not url.endswith('.json') and not url.endswith('.rss'):
            url = f"{url}.json"
        elif '/r/' in url and not url.endswith('.json') and not url.endswith('.rss') and '/comments/' not in url:
            url = f"{url}.json"
        
        return url
    
    def _convert_to_rss_url(self, url):
        """Chuy·ªÉn URL sang RSS format v·ªõi ?format=xml"""
        try:
            if '/comments/' in url:
                match = re.search(r'/r/([^/]+)/comments/([^/]+)', url)
                if match:
                    subreddit, post_id = match.groups()
                    return f"https://www.reddit.com/r/{subreddit}/comments/{post_id}.rss?format=xml"
            elif '/r/' in url:
                url = url.replace('.json', '').rstrip('/')
                return f"{url}.rss?format=xml"
        except:
            pass
        return None
    
    def _parse_rss_data(self, rss_content, url):
        """Parse RSS data"""
        try:
            import xml.etree.ElementTree as ET
            
            # Parse XML
            root = ET.fromstring(rss_content)
            
            # T√¨m title
            title = ""
            for item in root.findall('.//item'):
                title_elem = item.find('title')
                if title_elem is not None:
                    title = title_elem.text
                    break
            
            # T√¨m subreddit
            subreddit = "unknown"
            match = re.search(r'/r/([^/]+)', url)
            if match:
                subreddit = match.group(1)
            
            meta = {
                'title': title or 'No Title',
                'subreddit': subreddit,
                'score': 0,
                'author': 'Unknown',
                'content': 'Content from RSS feed',
                'upvote_ratio': 0,
                'created_utc': time.time(),
                'created_time': 'Kh√¥ng r√µ',
                'num_comments': 0,
                'permalink': url,
                'url': url,
                'id': 'rss_' + str(hash(url) % 10000)
            }
            
            return {'meta': meta, 'comments': []}, None
            
        except Exception as e:
            print(f"RSS parse error: {e}")
            return self._parse_html_fallback(rss_content, url)
    
    def _parse_reddit_data(self, data, original_url):
        """Parse d·ªØ li·ªáu Reddit JSON"""
        try:
            meta = {}
            comments = []
            
            if isinstance(data, list) and len(data) >= 2:
                post_part = data[0]
                if ('data' in post_part and 
                    'children' in post_part['data'] and 
                    len(post_part['data']['children']) > 0):
                    
                    post_data = post_part['data']['children'][0]['data']
                    meta = self._extract_post_meta(post_data, original_url)
                    
                    comments_part = data[1]
                    if ('data' in comments_part and 
                        'children' in comments_part['data']):
                        
                        comments_data = comments_part['data']['children']
                        comments = self._extract_comments(comments_data)
                
            elif isinstance(data, dict):
                if 'data' in data and 'children' in data['data']:
                    children = data['data']['children']
                    
                    if children and len(children) > 0:
                        item = children[0]
                        if 'data' in item:
                            item_data = item['data']
                            
                            if item.get('kind') == 't3':
                                meta = self._extract_post_meta(item_data, original_url)
                            elif item.get('kind') == 't1':
                                comments = self._extract_comments(children)
            
            if not meta:
                return None, "Kh√¥ng th·ªÉ ph√¢n t√≠ch d·ªØ li·ªáu b√†i vi·∫øt"
            
            return {'meta': meta, 'comments': comments}, None
            
        except Exception as e:
            print(f"‚ùå Parse error: {e}")
            return None, f"L·ªói ph√¢n t√≠ch: {str(e)}"
    
    def _extract_post_meta(self, post_data, original_url):
        """Tr√≠ch xu·∫•t metadata t·ª´ post data"""
        permalink = post_data.get('permalink', '')
        if permalink and not permalink.startswith('http'):
            permalink = f"https://www.reddit.com{permalink}"
        
        # T√≠nh th·ªùi gian ƒëƒÉng
        created_time = ""
        if 'created_utc' in post_data:
            try:
                post_time = datetime.fromtimestamp(post_data['created_utc'])
                now = datetime.now()
                diff = now - post_time
                
                if diff.days > 0:
                    created_time = f"{diff.days} ng√†y tr∆∞·ªõc"
                elif diff.seconds > 3600:
                    hours = diff.seconds // 3600
                    created_time = f"{hours} gi·ªù tr∆∞·ªõc"
                else:
                    minutes = diff.seconds // 60
                    created_time = f"{minutes} ph√∫t tr∆∞·ªõc"
            except:
                created_time = "Kh√¥ng r√µ"
        
        return {
            'title': post_data.get('title', 'No Title'),
            'subreddit': post_data.get('subreddit', 'unknown'),
            'score': post_data.get('score', 0),
            'author': post_data.get('author', '[deleted]'),
            'content': post_data.get('selftext', '')[:1500],
            'upvote_ratio': post_data.get('upvote_ratio', 0),
            'created_utc': post_data.get('created_utc', 0),
            'created_time': created_time,
            'num_comments': post_data.get('num_comments', 0),
            'permalink': permalink,
            'url': permalink or original_url,
            'id': post_data.get('id', ''),
        }
    
    def _extract_comments(self, comments_data):
        """Tr√≠ch xu·∫•t comments"""
        comments = []
        
        for child in comments_data:
            try:
                if child.get('kind') == 't1':
                    comment = child['data']
                    
                    if comment.get('body') in ['[deleted]', '[removed]']:
                        continue
                    
                    comments.append({
                        'body': comment.get('body', ''),
                        'author': comment.get('author', '[deleted]'),
                        'score': comment.get('score', 0),
                        'created_utc': comment.get('created_utc', 0),
                        'id': comment.get('id', ''),
                    })
            except Exception as e:
                continue
        
        return comments

    def _parse_html_fallback(self, html, url):
        """Fallback parse t·ª´ HTML"""
        try:
            title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
            title = title_match.group(1).strip() if title_match else 'No Title'
            title = title.replace(' : reddit', '').replace(' - Reddit', '').strip()
            
            subreddit_match = re.search(r'/r/([^/"\']+)', url)
            subreddit = subreddit_match.group(1) if subreddit_match else 'unknown'
            
            meta = {
                'title': title,
                'subreddit': subreddit,
                'score': 0,
                'author': 'Unknown',
                'content': 'Content available in HTML only',
                'url': url,
                'num_comments': 0,
                'permalink': url,
                'created_time': 'Kh√¥ng r√µ'
            }
            
            return {'meta': meta, 'comments': []}, None
            
        except Exception as e:
            return None, f"HTML parse error: {e}"

class TrendingManager:
    def __init__(self):
        self.mirrors = ["https://www.reddit.com", "https://old.reddit.com"]
        # USER-AGENT CHO TRENDING - T·∫†O NG·∫™U NHI√äN
        random_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
        self.user_agent_base = f'web:reddit_trending_fetcher_{random_id}:v1.0.0'
        
    def fetch_feed(self, subreddits, limit=15):
        """L·∫•y d·ªØ li·ªáu b√†i vi·∫øt t·ª´ subreddits v·ªõi fallback t·ª± ƒë·ªông"""
        results = []
        
        for sub in subreddits:
            sub = sub.strip().replace('r/', '').replace('/', '')
            
            # Strategy 1: Th·ª≠ c√°c mirrors
            posts = self._try_mirrors(sub, limit)
            
            # Strategy 2: Th·ª≠ RSS feed
            if not posts:
                posts = self._fetch_rss_feed(sub, limit)
            
            results.extend(posts)
        
        # S·∫Øp x·∫øp theo th·ªùi gian
        results.sort(key=lambda x: x.get('timestamp', 0), reverse=True)
        return results
    
    def _try_mirrors(self, subreddit, limit):
        """Th·ª≠ l·∫•y data qua c√°c mirrors"""
        for domain in self.mirrors:
            try:
                # T·∫°o User-Agent ng·∫´u nhi√™n cho m·ªói request
                random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))
                headers = {
                    'User-Agent': f'{self.user_agent_base}_{random_suffix}',
                    'Accept': 'application/json'
                }
                
                url = f"{domain}/r/{subreddit}/hot.json?limit={limit}"
                resp = requests.get(url, headers=headers, timeout=10)
                
                if resp.status_code == 200:
                    data = resp.json()
                    return self._parse_posts(data['data']['children'], subreddit)
                elif resp.status_code == 403:
                    # Th·ª≠ RSS tr√™n domain n√†y
                    rss_url = f"{domain}/r/{subreddit}/hot.rss?format=xml"
                    rss_resp = requests.get(rss_url, headers=headers, timeout=10)
                    if rss_resp.status_code == 200:
                        return self._parse_rss_feed(rss_resp.text, subreddit)
                        
            except Exception as e:
                print(f"Error fetching from {domain}: {e}")
                continue
        
        return []
    
    def _parse_rss_feed(self, rss_content, subreddit):
        """Parse RSS feed content"""
        try:
            import xml.etree.ElementTree as ET
            root = ET.fromstring(rss_content)
            
            posts = []
            for item in root.findall('.//item'):
                try:
                    title = item.find('title').text if item.find('title') is not None else 'No Title'
                    link = item.find('link').text if item.find('link') is not None else ''
                    
                    post_id = ''
                    if '/comments/' in link:
                        post_id = link.split('/comments/')[1].split('/')[0]
                    
                    author = 'Unknown'
                    author_elem = item.find('{http://purl.org/dc/elements/1.1/}creator')
                    if author_elem is not None:
                        author = author_elem.text
                    
                    post = {
                        'id': post_id or f"rss_{len(posts)}",
                        'title': title,
                        'url': link,
                        'subreddit': subreddit,
                        'author': author,
                        'score': 0,
                        'comments_count': 0,
                        'created_utc': time.time(),
                        'timestamp': time.time(),
                        'thumbnail': None,
                        'selftext': '',
                        'upvote_ratio': 0,
                        'time_str': datetime.now().strftime('%H:%M %d/%m')
                    }
                    posts.append(post)
                except:
                    continue
            
            return posts
            
        except Exception as e:
            print(f"RSS parse error: {e}")
            return []
    
    def _fetch_rss_feed(self, subreddit, limit):
        """Fallback s·ª≠ d·ª•ng RSS feed"""
        try:
            # Th√™m format=xml v√†o RSS URL
            rss_url = f"https://www.reddit.com/r/{subreddit}/hot.rss?format=xml"
            random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=4))
            headers = {
                'User-Agent': f'{self.user_agent_base}_{random_suffix}'
            }
            
            response = requests.get(rss_url, headers=headers, timeout=10)
            if response.status_code == 200:
                feed = feedparser.parse(response.text)
                posts = []
                for entry in feed.entries[:limit]:
                    post = {
                        'id': entry.id.split('/')[-1] if hasattr(entry, 'id') else f"rss_{len(posts)}",
                        'title': entry.title if hasattr(entry, 'title') else 'No Title',
                        'url': entry.link if hasattr(entry, 'link') else '',
                        'subreddit': subreddit,
                        'author': entry.author if hasattr(entry, 'author') else 'Unknown',
                        'score': 0,
                        'comments_count': 0,
                        'created_utc': time.mktime(entry.updated_parsed) if hasattr(entry, 'updated_parsed') else time.time(),
                        'timestamp': time.mktime(entry.updated_parsed) if hasattr(entry, 'updated_parsed') else time.time(),
                        'thumbnail': None,
                        'selftext': '',
                        'upvote_ratio': 0,
                        'time_str': datetime.now().strftime('%H:%M %d/%m')
                    }
                    posts.append(post)
                return posts
        except:
            pass
        return []
    
    def _parse_posts(self, posts_data, subreddit):
        """Parse d·ªØ li·ªáu b√†i vi·∫øt t·ª´ JSON response"""
        posts = []
        for child in posts_data:
            p = child['data']
            try:
                thumb = None
                if p.get('thumbnail') and p['thumbnail'].startswith('http'):
                    thumb = p['thumbnail']
                elif p.get('preview'):
                    try:
                        thumb = p['preview']['images'][0]['source']['url'].replace('&amp;', '&')
                    except:
                        pass
                
                post = {
                    'id': p['id'],
                    'title': p.get('title', 'No Title'),
                    'url': f"https://www.reddit.com{p['permalink']}",
                    'subreddit': subreddit,
                    'author': p.get('author', '[deleted]'),
                    'score': p.get('score', 0),
                    'comments_count': p.get('num_comments', 0),
                    'created_utc': p.get('created_utc', time.time()),
                    'timestamp': p.get('created_utc', time.time()),
                    'thumbnail': thumb,
                    'selftext': p.get('selftext', ''),
                    'upvote_ratio': p.get('upvote_ratio', 0),
                    'time_str': datetime.fromtimestamp(p.get('created_utc', time.time())).strftime('%H:%M %d/%m')
                }
                posts.append(post)
            except Exception:
                continue
        return posts

# FORECAST ENGINE N√ÇNG CAO
class AdvancedForecastEngine:
    """Forecast engine v·ªõi nhi·ªÅu bi·ªÉu ƒë·ªì"""
    
    def forecast(self, posts_data, days=7):
        if not posts_data or len(posts_data) < 3:
            return self._get_empty_forecast()
        
        try:
            # T√≠nh engagement v√† c√°c ch·ªâ s·ªë
            engagements = []
            scores = []
            comments = []
            timestamps = []
            
            for post in posts_data:
                engagement = post.get('score', 0) + post.get('comments_count', 0) * 2
                engagements.append(engagement)
                scores.append(post.get('score', 0))
                comments.append(post.get('comments_count', 0))
                timestamps.append(post.get('created_utc', time.time()))
            
            # Ph√¢n t√≠ch trend
            avg_engagement = np.mean(engagements)
            std_engagement = np.std(engagements)
            
            # X√°c ƒë·ªãnh xu h∆∞·ªõng
            if len(engagements) >= 5:
                recent_avg = np.mean(engagements[-3:])
                older_avg = np.mean(engagements[:3])
                
                if recent_avg > older_avg * 1.3:
                    trend = "TƒÉng m·∫°nh üöÄ"
                    trend_slope = 0.03
                elif recent_avg > older_avg * 1.1:
                    trend = "TƒÉng nh·∫π ‚ÜóÔ∏è"
                    trend_slope = 0.015
                elif recent_avg < older_avg * 0.7:
                    trend = "Gi·∫£m m·∫°nh üìâ"
                    trend_slope = -0.03
                elif recent_avg < older_avg * 0.9:
                    trend = "Gi·∫£m nh·∫π ‚ÜòÔ∏è"
                    trend_slope = -0.015
                else:
                    trend = "·ªîn ƒë·ªãnh ‚û°Ô∏è"
                    trend_slope = 0.0
            else:
                trend = "ƒêang ph√¢n t√≠ch üìä"
                trend_slope = 0.02
            
            # T·∫°o forecast data
            forecast_data = []
            today = datetime.now()
            
            for i in range(min(days, 7)):
                future_date = today + timedelta(days=i+1)
                
                # D·ª± b√°o v·ªõi nhi·ªÖu ng·∫´u nhi√™n nh·ªè
                base_prediction = avg_engagement * (1 + trend_slope) ** (i + 1)
                noise = np.random.normal(0, std_engagement * 0.1)
                predicted = max(10, base_prediction + noise)
                
                forecast_data.append({
                    'date': future_date.strftime('%Y-%m-%d'),
                    'predicted_engagement': round(predicted, 1),
                    'predicted_lower': round(predicted * 0.8, 1),
                    'predicted_upper': round(predicted * 1.2, 1),
                    'confidence': 'medium'
                })
            
            # T·∫°o d·ªØ li·ªáu cho bi·ªÉu ƒë·ªì l·ªãch s·ª≠
            history_data = []
            for i, (eng, ts) in enumerate(zip(engagements, timestamps)):
                try:
                    date_str = datetime.fromtimestamp(ts).strftime('%m/%d')
                    history_data.append({
                        'date': date_str,
                        'engagement': eng,
                        'score': scores[i],
                        'comments': comments[i]
                    })
                except:
                    continue
            
            return {
                'forecast': forecast_data,
                'history': history_data[:10],  # L·∫•y 10 ƒëi·ªÉm g·∫ßn nh·∫•t
                'trend_direction': trend,
                'trend_slope': trend_slope,
                'current_stats': {
                    'avg_engagement': float(avg_engagement),
                    'std_engagement': float(std_engagement),
                    'max_engagement': float(np.max(engagements)),
                    'min_engagement': float(np.min(engagements)),
                    'total_posts': len(posts_data)
                },
                'confidence_interval': 'medium',
                'method_used': 'advanced_regression'
            }
            
        except Exception as e:
            print(f"Forecast error: {e}")
            return self._get_empty_forecast()
    
    def _get_empty_forecast(self):
        """T·∫°o forecast m·∫∑c ƒë·ªãnh khi kh√¥ng c√≥ d·ªØ li·ªáu"""
        today = datetime.now()
        forecast_data = []
        
        for i in range(7):
            future_date = today + timedelta(days=i+1)
            forecast_data.append({
                'date': future_date.strftime('%Y-%m-%d'),
                'predicted_engagement': 50 + i * 5,
                'predicted_lower': 30 + i * 3,
                'predicted_upper': 70 + i * 7,
                'confidence': 'low'
            })
        
        return {
            'forecast': forecast_data,
            'history': [],
            'trend_direction': 'ƒêang ph√¢n t√≠ch üìä',
            'trend_slope': 0.02,
            'current_stats': {
                'avg_engagement': 50,
                'std_engagement': 10,
                'max_engagement': 100,
                'min_engagement': 10,
                'total_posts': 0
            },
            'confidence_interval': 'low',
            'method_used': 'default'
        }

class TrendAnalysisManager:
    def __init__(self):
        self.forecast_engine = AdvancedForecastEngine()
    
    def analyze_subreddit_trends(self, subreddit, posts_data, days=7):
        """Ph√¢n t√≠ch xu h∆∞·ªõng chi ti·∫øt"""
        
        # Ki·ªÉm tra cache
        cached_data = db.get_cached_trend_data(subreddit)
        if cached_data:
            return cached_data
        
        # S·ª≠ d·ª•ng AdvancedForecastEngine
        forecast_result = self.forecast_engine.forecast(posts_data, days)
        
        # T·∫°o k·∫øt qu·∫£ ho√†n ch·ªânh
        result = {
            'subreddit': subreddit,
            'analysis_period_days': days,
            'data_summary': self._calculate_basic_summary(posts_data),
            'peak_hours': self._calculate_peak_hours(posts_data),
            'top_keywords': self._extract_simple_keywords(posts_data),
            'top_topics': self._extract_topics(posts_data),
            'forecast': forecast_result,
            'analysis_timestamp': datetime.now().isoformat(),
            'note': 'Ph√¢n t√≠ch n√¢ng cao v·ªõi d·ª± b√°o 7 ng√†y üìä'
        }
        
        # Cache k·∫øt qu·∫£
        db.cache_trend_data(subreddit, result)
        
        return result

    def _calculate_basic_summary(self, posts_data):
        """T√≠nh summary c∆° b·∫£n"""
        if not posts_data:
            return {}
            
        total_posts = len(posts_data)
        total_score = sum(p.get('score', 0) for p in posts_data)
        total_comments = sum(p.get('comments_count', 0) for p in posts_data)
        total_engagement = total_score + total_comments * 2
        
        # T√≠nh ƒë·ªô bi·∫øn ƒë·ªông
        engagements = [p.get('score', 0) + p.get('comments_count', 0) * 2 for p in posts_data]
        volatility = np.std(engagements) if len(engagements) > 1 else 0
        
        return {
            'total_posts_analyzed': total_posts,
            'total_score': int(total_score),
            'total_engagement': int(total_engagement),
            'total_comments': int(total_comments),
            'avg_score_per_post': float(total_score / total_posts) if total_posts > 0 else 0,
            'avg_comments_per_post': float(total_comments / total_posts) if total_posts > 0 else 0,
            'avg_engagement_per_post': float(total_engagement / total_posts) if total_posts > 0 else 0,
            'volatility': float(volatility),
            'engagement_range': f"{min(engagements) if engagements else 0} - {max(engagements) if engagements else 0}"
        }

    def _calculate_peak_hours(self, posts_data):
        """T√≠nh gi·ªù cao ƒëi·ªÉm t·ª´ posts data"""
        if not posts_data:
            return []
            
        try:
            hour_engagement = {}
            hour_posts = {}
            
            for post in posts_data:
                try:
                    hour = datetime.fromtimestamp(post['created_utc']).hour
                    engagement = post.get('score', 0) + post.get('comments_count', 0) * 2
                    
                    if hour not in hour_engagement:
                        hour_engagement[hour] = 0
                        hour_posts[hour] = 0
                    
                    hour_engagement[hour] += engagement
                    hour_posts[hour] += 1
                except:
                    continue
            
            peak_hours = []
            for hour in sorted(hour_engagement.keys()):
                peak_hours.append({
                    'hour': int(hour),
                    'total_engagement': int(hour_engagement[hour]),
                    'post_count': int(hour_posts.get(hour, 0)),
                    'avg_engagement': int(hour_engagement[hour] / hour_posts[hour]) if hour_posts.get(hour, 0) > 0 else 0
                })
            
            return sorted(peak_hours, key=lambda x: x['total_engagement'], reverse=True)
            
        except Exception as e:
            print(f"Peak hours error: {e}")
            return []

    def _extract_simple_keywords(self, posts_data, top_n=10):
        """Tr√≠ch xu·∫•t keywords"""
        if not posts_data:
            return []
            
        try:
            all_titles = " ".join([str(p.get('title', '')) for p in posts_data])
            words = re.findall(r'\b[a-zA-Z√Ä-·ªπ]{4,}\b', all_titles.lower())
            
            stopwords = {'the', 'and', 'for', 'with', 'this', 'that', 'have', 'from', 'they', 'what', 
                        'about', 'when', 'where', 'which', 'would', 'could', 'should', 'their'}
            filtered_words = [w for w in words if w not in stopwords]
            
            word_counts = Counter(filtered_words).most_common(top_n)
            
            return [{'keyword': w.capitalize(), 'score': c/len(filtered_words) if filtered_words else 0} 
                   for w, c in word_counts]
                   
        except Exception as e:
            print(f"Keywords error: {e}")
            return []

    def _extract_topics(self, posts_data, top_n=5):
        """Tr√≠ch xu·∫•t ch·ªß ƒë·ªÅ t·ª´ n·ªôi dung"""
        if not posts_data:
            return []
        
        try:
            # S·ª≠ d·ª•ng keywords l√†m ch·ªß ƒë·ªÅ ƒë∆°n gi·∫£n
            keywords = self._extract_simple_keywords(posts_data, top_n=top_n*2)
            
            topics = []
            for kw in keywords[:top_n]:
                topics.append({
                    'topic': kw['keyword'],
                    'relevance': kw['score'],
                    'posts_count': sum(1 for p in posts_data if kw['keyword'].lower() in p.get('title', '').lower())
                })
            
            return sorted(topics, key=lambda x: x['relevance'], reverse=True)
        except:
            return []

# ==========================================
# AI ANALYST N√ÇNG CAO V·ªöI GEMINI 2.0 FLASH
# ==========================================
class AdvancedAIAnalyst:
    def __init__(self):
        self.key = GOOGLE_GEMINI_API_KEY
        self.last_request_time = None
        self.request_count = 0
        self.cooldown_until = None
        
    def analyze(self, meta, comments):
        """Ph√¢n t√≠ch AI n√¢ng cao v·ªõi Gemini 2.0 Flash"""
        
        # 1. Ki·ªÉm tra ƒëi·ªÅu ki·ªán c∆° b·∫£n
        if not self.key:
            return self._get_comprehensive_fallback(meta, comments, "Ch∆∞a c·∫•u h√¨nh API key")
        
        if not GEMINI_AVAILABLE:
            return self._get_comprehensive_fallback(meta, comments, "Th∆∞ vi·ªán Gemini ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        
        # 2. Ki·ªÉm tra rate limiting
        if self._should_use_fallback():
            return self._get_comprehensive_fallback(meta, comments, "AI ƒëang b·∫≠n, vui l√≤ng th·ª≠ l·∫°i sau")
        
        # 3. Th·ª≠ g·ªçi API
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.key)
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu
            analysis_data = self._prepare_analysis_data(meta, comments)
            
            # T·∫°o prompt chi ti·∫øt v·ªõi y√™u c·∫ßu t√≥m t·∫Øt
            prompt = self._create_detailed_prompt(analysis_data)
            
            # G·ªçi Gemini 2.0 Flash
            try:
                model = genai.GenerativeModel('gemini-2.0-flash')
                
                # Update tracking
                self.last_request_time = datetime.now()
                self.request_count += 1
                
                # G·ªçi API
                response = model.generate_content(
                    prompt,
                    generation_config={
                        "temperature": 0.7,
                        "max_output_tokens": 800,  # TƒÉng l√™n ƒë·ªÉ c√≥ t√≥m t·∫Øt chi ti·∫øt
                        "top_p": 0.95,
                        "top_k": 40
                    },
                    safety_settings=[
                        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                    ]
                )
                
                if response and response.text:
                    return self._format_ai_response(response.text, analysis_data)
                else:
                    return self._get_comprehensive_fallback(meta, comments, "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI")
                    
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg or "quota" in error_msg.lower():
                    self.cooldown_until = datetime.now() + timedelta(minutes=5)
                    return self._get_comprehensive_fallback(meta, comments, "ƒêang s·ª≠ d·ª•ng ph√¢n t√≠ch c∆° b·∫£n do gi·ªõi h·∫°n API")
                else:
                    return self._get_comprehensive_fallback(meta, comments, f"L·ªói AI: {error_msg[:100]}")
                
        except Exception as e:
            return self._get_comprehensive_fallback(meta, comments, f"L·ªói h·ªá th·ªëng: {str(e)[:100]}")
    
    def _should_use_fallback(self):
        """Ki·ªÉm tra c√≥ n√™n d√πng fallback kh√¥ng"""
        # N·∫øu ƒëang trong cooldown
        if self.cooldown_until and datetime.now() < self.cooldown_until:
            return True
        
        # N·∫øu ƒë√£ g·ªçi qu√° nhi·ªÅu request
        if self.request_count >= 10:
            if self.last_request_time:
                time_diff = (datetime.now() - self.last_request_time).total_seconds()
                if time_diff < 300:  # 5 ph√∫t
                    return True
        
        return False
    
    def _prepare_analysis_data(self, meta, comments):
        """Chu·∫©n b·ªã d·ªØ li·ªáu ph√¢n t√≠ch"""
        # Ph√¢n t√≠ch sentiment t·ª´ comments
        sentiments = []
        emotion_counts = {}
        
        for c in comments[:20]:  # Ch·ªâ ph√¢n t√≠ch 20 comments ƒë·∫ßu
            try:
                blob = TextBlob(c['body'])
                pol = blob.sentiment.polarity
                
                if pol > 0.3:
                    sentiment = "R·∫•t t√≠ch c·ª±c"
                    emotion = "üòä H√†i l√≤ng"
                elif pol > 0.1:
                    sentiment = "T√≠ch c·ª±c"
                    emotion = "üôÇ Vui v·∫ª"
                elif pol < -0.3:
                    sentiment = "R·∫•t ti√™u c·ª±c"
                    emotion = "üò† T·ª©c gi·∫≠n"
                elif pol < -0.1:
                    sentiment = "Ti√™u c·ª±c"
                    emotion = "üòü Lo l·∫Øng"
                else:
                    sentiment = "Trung l·∫≠p"
                    emotion = "üòê B√¨nh th∆∞·ªùng"
                
                sentiments.append({
                    'sentiment': sentiment,
                    'emotion': emotion,
                    'polarity': pol,
                    'text': c['body'][:100] + '...' if len(c['body']) > 100 else c['body'],
                    'score': c.get('score', 0)
                })
                
                # ƒê·∫øm emotion
                if emotion in emotion_counts:
                    emotion_counts[emotion] += 1
                else:
                    emotion_counts[emotion] = 1
                    
            except:
                continue
        
        # T√≠nh to√°n th·ªëng k√™
        total_comments = len(comments)
        analyzed_comments = len(sentiments)
        
        # Top comments
        top_comments = sorted(comments, key=lambda x: x.get('score', 0), reverse=True)[:3]
        
        # T·∫°o t√≥m t·∫Øt n·ªôi dung
        content_summary = self._create_content_summary(meta, comments)
        
        return {
            'meta': meta,
            'total_comments': total_comments,
            'analyzed_comments': analyzed_comments,
            'sentiments': sentiments,
            'emotion_counts': emotion_counts,
            'top_comments': top_comments,
            'engagement_score': meta.get('score', 0) + total_comments * 2,
            'content_summary': content_summary
        }
    
    def _create_content_summary(self, meta, comments):
        """T·∫°o t√≥m t·∫Øt n·ªôi dung b√†i vi·∫øt"""
        try:
            # L·∫•y n·ªôi dung ch√≠nh
            title = meta.get('title', '')
            content = meta.get('content', '')
            
            # N·∫øu content qu√° ng·∫Øn, l·∫•y th√™m t·ª´ comments
            if len(content) < 100 and comments:
                # L·∫•y 3 comments h√†ng ƒë·∫ßu
                top_comments_text = ' '.join([c['body'][:200] for c in comments[:3]])
                full_text = f"{title}. {content}. {top_comments_text}"
            else:
                full_text = f"{title}. {content}"
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i
            if len(full_text) > 1500:
                full_text = full_text[:1500] + "..."
            
            # T·∫°o t√≥m t·∫Øt ƒë∆°n gi·∫£n
            words = full_text.split()
            if len(words) > 100:
                # L·∫•y c√¢u ƒë·∫ßu v√† cu·ªëi
                sentences = re.split(r'[.!?]+', full_text)
                if len(sentences) > 2:
                    summary = sentences[0] + ". " + sentences[-2] + "."
                else:
                    summary = ' '.join(words[:50]) + "..."
            else:
                summary = full_text
            
            return summary[:500] + "..." if len(summary) > 500 else summary
            
        except Exception as e:
            print(f"Error creating summary: {e}")
            return meta.get('title', '')[:200]
    
    def _create_detailed_prompt(self, data):
        """T·∫°o prompt chi ti·∫øt cho AI v·ªõi y√™u c·∫ßu t√≥m t·∫Øt"""
        meta = data['meta']
        
        prompt = f"""H√£y ph√¢n t√≠ch b√†i ƒëƒÉng Reddit sau ƒë√¢y b·∫±ng ti·∫øng Vi·ªát:

**TH√îNG TIN B√ÄI ƒêƒÇNG:**
- Ti√™u ƒë·ªÅ: {meta.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}
- Subreddit: r/{meta.get('subreddit', 'unknown')}
- T√°c gi·∫£: {meta.get('author', '·∫®n danh')}
- ƒêi·ªÉm: {meta.get('score', 0)}
- S·ªë b√¨nh lu·∫≠n: {data['total_comments']}
- T·ª∑ l·ªá upvote: {meta.get('upvote_ratio', 0):.1%}
- Engagement: {data['engagement_score']}

**N·ªòI DUNG CH√çNH (ƒê√É T√ìM T·∫ÆT):**
{data['content_summary']}

**TH·ªêNG K√ä C·∫¢M X√öC ({data['analyzed_comments']}/{data['total_comments']} b√¨nh lu·∫≠n):**
{self._format_sentiment_stats(data['emotion_counts'])}

**Y√äU C·∫¶U PH√ÇN T√çCH:**
H√£y cung c·∫•p ph√¢n t√≠ch v·ªõi c√°c ph·∫ßn sau:

1. **T√ìM T·∫ÆT CHI TI·∫æT N·ªòI DUNG** (4-5 c√¢u b·∫±ng ti·∫øng Vi·ªát):
   - T√≥m t·∫Øt √Ω ch√≠nh c·ªßa b√†i vi·∫øt
   - M·ª•c ƒë√≠ch ch√≠nh c·ªßa t√°c gi·∫£
   - Th√¥ng tin quan tr·ªçng nh·∫•t

2. **PH√ÇN T√çCH C·∫¢M X√öC C·ªòNG ƒê·ªíNG**:
   - Xu h∆∞·ªõng c·∫£m x√∫c chung
   - ƒêi·ªÉm ƒë√°ng ch√∫ √Ω v·ªÅ ph·∫£n ·ª©ng c·ªßa c·ªông ƒë·ªìng
   - M·ª©c ƒë·ªô tham gia th·∫£o lu·∫≠n

3. **ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG**:
   - Ch·∫•t l∆∞·ª£ng n·ªôi dung b√†i ƒëƒÉng
   - M·ª©c ƒë·ªô t∆∞∆°ng t√°c (engagement)
   - Ti·ªÅm nƒÉng viral (n·∫øu c√≥)

4. **KHUY·∫æN NGH·ªä**:
   - Khuy·∫øn ngh·ªã cho t√°c gi·∫£ (n·∫øu c·∫ßn)
   - Th·ªùi ƒëi·ªÉm t·ªët nh·∫•t ƒë·ªÉ tham gia th·∫£o lu·∫≠n

**L∆ØU √ù:** H√£y vi·∫øt ng·∫Øn g·ªçn, s√∫c t√≠ch, t·∫≠p trung v√†o insights c√≥ gi√° tr·ªã. ∆Øu ti√™n t√≥m t·∫Øt n·ªôi dung r√µ r√†ng.

**PH√ÇN T√çCH:**"""
        
        return prompt
    
    def _format_sentiment_stats(self, emotion_counts):
        """ƒê·ªãnh d·∫°ng th·ªëng k√™ c·∫£m x√∫c"""
        if not emotion_counts:
            return "Kh√¥ng c√≥ d·ªØ li·ªáu c·∫£m x√∫c"
        
        stats = []
        total = sum(emotion_counts.values())
        
        for emotion, count in emotion_counts.items():
            percentage = (count / total) * 100 if total > 0 else 0
            stats.append(f"- {emotion}: {count} ({percentage:.1f}%)")
        
        return "\n".join(stats)
    
    def _format_ai_response(self, response_text, data):
        """ƒê·ªãnh d·∫°ng ph·∫£n h·ªìi AI"""
        # Th√™m header cho ph√¢n t√≠ch AI
        formatted = f"""
## ü§ñ PH√ÇN T√çCH AI CHI TI·∫æT
        
**üìä Th√¥ng tin b√†i ƒëƒÉng:**
- **Ti√™u ƒë·ªÅ:** {data['meta'].get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}
- **Subreddit:** r/{data['meta'].get('subreddit', 'unknown')}
- **Engagement:** {data['engagement_score']} ƒëi·ªÉm
- **Ph√¢n t√≠ch:** {data['analyzed_comments']}/{data['total_comments']} b√¨nh lu·∫≠n

---

{response_text}

---

*Ph√¢n t√≠ch ƒë∆∞·ª£c th·ª±c hi·ªán b·ªüi Gemini 2.0 Flash ‚Ä¢ {datetime.now().strftime("%H:%M %d/%m/%Y")}*
"""
        return formatted
    
    def _get_comprehensive_fallback(self, meta, comments, reason=""):
        """Fallback ph√¢n t√≠ch chi ti·∫øt v·ªõi t√≥m t·∫Øt n·ªôi dung"""
        title = meta.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')
        score = meta.get('score', 0)
        upvote_ratio = meta.get('upvote_ratio', 0)
        num_comments = len(comments)
        engagement = score + num_comments * 2
        
        # T·∫°o t√≥m t·∫Øt n·ªôi dung
        content_summary = self._create_content_summary(meta, comments)
        
        # Ph√¢n t√≠ch sentiment c∆° b·∫£n
        sentiment_stats = self._analyze_comments_sentiment_basic(comments)
        
        # X√°c ƒë·ªãnh ch·∫•t l∆∞·ª£ng b√†i ƒëƒÉng
        quality = "T·ªët" if engagement > 100 else "Trung b√¨nh" if engagement > 30 else "Th·∫•p"
        viral_potential = "Cao" if engagement > 300 else "Trung b√¨nh" if engagement > 100 else "Th·∫•p"
        
        # T·∫°o ph√¢n t√≠ch fallback chi ti·∫øt
        analysis = f"""
## üìä PH√ÇN T√çCH C∆† B·∫¢N

**üîç TH√îNG TIN B√ÄI ƒêƒÇNG:**
- **Ti√™u ƒë·ªÅ:** {title[:80]}...
- **Subreddit:** r/{meta.get('subreddit', 'unknown')}
- **T√°c gi·∫£:** {meta.get('author', '·∫®n danh')}
- **Th·ªùi gian:** {meta.get('created_time', 'Kh√¥ng r√µ')}

**üìà CH·ªà S·ªê T∆Ø∆†NG T√ÅC:**
- **ƒêi·ªÉm:** {score} ‚≠ê
- **T·ª∑ l·ªá upvote:** {upvote_ratio:.1%} üìä
- **B√¨nh lu·∫≠n:** {num_comments} üí¨
- **Engagement:** {engagement} üìà
- **Ch·∫•t l∆∞·ª£ng:** {quality} 
- **Ti·ªÅm nƒÉng viral:** {viral_potential}

### üìù T√ìM T·∫ÆT N·ªòI DUNG (Ti·∫øng Vi·ªát)
{content_summary}

**üé≠ PH√ÇN T√çCH C·∫¢M X√öC:**
{sentiment_stats}

**üí° NH·∫¨N X√âT CHUNG:**
- B√†i ƒëƒÉng c√≥ m·ª©c ƒë·ªô t∆∞∆°ng t√°c **{quality.lower()}**
- C·ªông ƒë·ªìng ƒëang c√≥ ph·∫£n ·ª©ng **{'t√≠ch c·ª±c' if 'üòä' in sentiment_stats else 'trung l·∫≠p' if 'üòê' in sentiment_stats else 'ti√™u c·ª±c'}**
- {'C√≥ ti·ªÅm nƒÉng thu h√∫t th√™m t∆∞∆°ng t√°c' if viral_potential == 'Cao' else 'C·∫ßn c·∫£i thi·ªán ƒë·ªÉ tƒÉng t∆∞∆°ng t√°c'}

"""
        return analysis
    
    def _analyze_comments_sentiment_basic(self, comments):
        """Ph√¢n t√≠ch sentiment c∆° b·∫£n t·ª´ comments"""
        if not comments:
            return "Kh√¥ng c√≥ b√¨nh lu·∫≠n ƒë·ªÉ ph√¢n t√≠ch"
        
        sentiments = []
        for c in comments[:10]:
            try:
                blob = TextBlob(c['body'])
                pol = blob.sentiment.polarity
                
                if pol > 0.1:
                    sentiments.append('positive')
                elif pol < -0.1:
                    sentiments.append('negative')
                else:
                    sentiments.append('neutral')
            except:
                continue
        
        if not sentiments:
            return "Kh√¥ng th·ªÉ ph√¢n t√≠ch c·∫£m x√∫c"
        
        pos = sentiments.count('positive')
        neg = sentiments.count('negative')
        neu = sentiments.count('neutral')
        total = len(sentiments)
        
        return f"""
- üòä **T√≠ch c·ª±c:** {pos} ({pos/total*100:.1f}%)
- üòê **Trung l·∫≠p:** {neu} ({neu/total*100:.1f}%)
- üòü **Ti√™u c·ª±c:** {neg} ({neg/total*100:.1f}%)
"""

# ==========================================
# DATA PROCESSING & EXPORT
# ==========================================
def process_nlp(comments):
    """X·ª≠ l√Ω NLP chi ti·∫øt cho comments"""
    if not comments:
        return pd.DataFrame()
    
    data = []
    for idx, c in enumerate(comments):
        try:
            blob = TextBlob(c['body'])
            pol = blob.sentiment.polarity
            subj = blob.sentiment.subjectivity
            
            # X√°c ƒë·ªãnh sentiment
            if pol > 0.3:
                sent = 'R·∫•t t√≠ch c·ª±c'
                emoji = 'üòä'
                color = '#4CAF50'
            elif pol > 0.1:
                sent = 'T√≠ch c·ª±c'
                emoji = 'üôÇ'
                color = '#8BC34A'
            elif pol < -0.3:
                sent = 'R·∫•t ti√™u c·ª±c'
                emoji = 'üò†'
                color = '#F44336'
            elif pol < -0.1:
                sent = 'Ti√™u c·ª±c'
                emoji = 'üòü'
                color = '#FF9800'
            else:
                sent = 'Trung l·∫≠p'
                emoji = 'üòê'
                color = '#9E9E9E'
            
            # X√°c ƒë·ªãnh emotion t·ª´ t·ª´ kh√≥a
            txt = c['body'].lower()
            emotion = 'B√¨nh th∆∞·ªùng'
            if any(x in txt for x in ['love', 'amazing', 'perfect', 'excellent', 'best']):
                emotion = 'Y√™u th√≠ch ‚ù§Ô∏è'
            elif any(x in txt for x in ['hate', 'terrible', 'worst', 'awful', 'bad']):
                emotion = 'Gh√©t b·ªè üíî'
            elif any(x in txt for x in ['lol', 'haha', 'funny', 'hilarious']):
                emotion = 'Vui v·∫ª üòÇ'
            elif any(x in txt for x in ['sad', 'sorry', 'unfortunately', 'bad news']):
                emotion = 'Bu·ªìn b√£ üò¢'
            
            data.append({
                'id': idx + 1,
                'comment_id': c.get('id', f'c{idx}'),
                'author': c.get('author', '[deleted]'),
                'text': c['body'][:200] + '...' if len(c['body']) > 200 else c['body'],
                'score': c.get('score', 0),
                'polarity': round(pol, 3),
                'subjectivity': round(subj, 3),
                'sentiment': sent,
                'sentiment_emoji': emoji,
                'sentiment_color': color,
                'emotion': emotion,
                'word_count': len(c['body'].split()),
                'char_count': len(c['body'])
            })
        except Exception as e:
            print(f"Error processing comment {idx}: {e}")
            continue
    
    return pd.DataFrame(data)

def create_download_link(df, filename="sentiment_analysis.csv"):
    """T·∫°o link download CSV"""
    csv = df.to_csv(index=False)
    b64 = base64.b64encode(csv.encode()).decode()
    href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">üì• T·∫£i xu·ªëng CSV</a>'
    return href

def create_visualization(df):
    """T·∫°o visualization cho sentiment analysis"""
    if df.empty:
        return None
    
    # 1. Pie chart ph√¢n b·ªë sentiment
    sentiment_counts = df['sentiment'].value_counts()
    
    fig1 = go.Figure(data=[go.Pie(
        labels=sentiment_counts.index,
        values=sentiment_counts.values,
        hole=.3,
        marker=dict(colors=df.drop_duplicates('sentiment').set_index('sentiment').loc[sentiment_counts.index, 'sentiment_color'].tolist())
    )])
    
    fig1.update_layout(
        title='Ph√¢n b·ªë c·∫£m x√∫c',
        height=400,
        showlegend=True
    )
    
    # 2. Bar chart sentiment theo ƒëi·ªÉm
    if 'score' in df.columns:
        fig2 = px.bar(
            df.nlargest(10, 'score'),
            x='author',
            y='score',
            color='sentiment',
            title='Top 10 b√¨nh lu·∫≠n ƒëi·ªÉm cao nh·∫•t',
            labels={'author': 'T√°c gi·∫£', 'score': 'ƒêi·ªÉm'},
            color_discrete_map=dict(zip(
                df['sentiment'].unique(),
                df.drop_duplicates('sentiment').set_index('sentiment')['sentiment_color'].tolist()
            ))
        )
        fig2.update_layout(height=400, xaxis_tickangle=-45)
    
    # 3. Scatter plot polarity vs subjectivity
    fig3 = px.scatter(
        df,
        x='polarity',
        y='subjectivity',
        color='sentiment',
        size='score',
        hover_data=['author', 'text'],
        title='Ph√¢n b·ªë c·∫£m x√∫c (Polarity vs Subjectivity)',
        labels={'polarity': 'C·ª±c t√≠nh', 'subjectivity': 'Ch·ªß quan'},
        color_discrete_map=dict(zip(
            df['sentiment'].unique(),
            df.drop_duplicates('sentiment').set_index('sentiment')['sentiment_color'].tolist()
        ))
    )
    fig3.update_layout(height=500)
    
    return fig1, fig2, fig3

# ==========================================
# 4. PAGE CONTROLLERS - GIAO DI·ªÜN ƒê∆†N GI·∫¢N
# ==========================================

def login_page():
    ui.render_login_screen()
    t1, t2 = st.tabs(["ƒêƒÉng nh·∫≠p", "ƒêƒÉng k√Ω"])
    with t1:
        with st.form("login"):
            u = st.text_input("T√™n ƒëƒÉng nh·∫≠p")
            p = st.text_input("M·∫≠t kh·∫©u", type="password")
            if st.form_submit_button("ƒêƒÉng nh·∫≠p", use_container_width=True):
                user = db.login(u, p)
                if user:
                    st.session_state.user = {"id": user[0], "username": user[1]}
                    st.session_state.authenticated = True
                    st.session_state.page = "Dashboard"
                    st.rerun()
                else: 
                    st.error("Sai t√™n ƒëƒÉng nh·∫≠p ho·∫∑c m·∫≠t kh·∫©u")
    with t2:
        with st.form("reg"):
            u = st.text_input("T√™n ng∆∞·ªùi d√πng m·ªõi")
            p = st.text_input("M·∫≠t kh·∫©u m·ªõi", type="password")
            if st.form_submit_button("ƒêƒÉng k√Ω", use_container_width=True):
                if len(u) < 3:
                    st.error("T√™n ng∆∞·ªùi d√πng ph·∫£i c√≥ √≠t nh·∫•t 3 k√Ω t·ª±")
                elif len(p) < 6:
                    st.error("M·∫≠t kh·∫©u ph·∫£i c√≥ √≠t nh·∫•t 6 k√Ω t·ª±")
                elif db.register(u, p): 
                    st.success("ƒêƒÉng k√Ω th√†nh c√¥ng! H√£y ƒëƒÉng nh·∫≠p.")
                else: 
                    st.error("T√™n ng∆∞·ªùi d√πng ƒë√£ t·ªìn t·∫°i")

def dashboard_page():
    user = st.session_state.user
    history = db.get_history(user['id'])
    ui.render_dashboard_header(user['username'])
    
    # Th·ªëng k√™ nhanh
    groups = db.get_groups(user['id'])
    st.markdown(f"### üìã T·ªïng quan")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üë• Nh√≥m theo d√µi", len(groups))
    with col2:
        st.metric("üìù L·ªãch s·ª≠ ph√¢n t√≠ch", len(history))
    with col3:
        last_update = st.session_state.get('last_update')
        if isinstance(last_update, datetime):
            st.metric("üîÑ C·∫≠p nh·∫≠t cu·ªëi", last_update.strftime('%H:%M'))
        else:
            st.metric("üîÑ C·∫≠p nh·∫≠t cu·ªëi", "Ch∆∞a c√≥")
    
    st.divider()
    
    # Feature cards
    st.markdown("### üöÄ T√≠nh nƒÉng ch√≠nh")
    c1, c2 = st.columns(2)
    with c1: 
        ui.render_feature_card(
            "üìä", "Ph√¢n T√≠ch Xu H∆∞·ªõng", 
            "Ph√¢n t√≠ch AI chuy√™n s√¢u c√°c c·ªông ƒë·ªìng Reddit.", 
            "btn_tr", "Kh√°m Ph√° Ngay", 
            lambda: (setattr(st.session_state, 'page', 'Trending'), st.rerun())
        )
    with c2: 
        ui.render_feature_card(
            "üîó", "Ph√¢n T√≠ch B√†i Vi·∫øt", 
            "Ph√¢n t√≠ch chi ti·∫øt b√†i vi·∫øt v√† b√¨nh lu·∫≠n.", 
            "btn_an", "Ph√¢n T√≠ch", 
            lambda: (setattr(st.session_state, 'page', 'Analysis'), st.rerun())
        )
    
    st.divider()
    
    # L·ªãch s·ª≠
    if history:
        st.markdown("### üìú L·ªãch s·ª≠ ph√¢n t√≠ch g·∫ßn ƒë√¢y")
        ui.render_history_list(history, db.delete_history)
    else:
        st.info("üìù Ch∆∞a c√≥ l·ªãch s·ª≠ ph√¢n t√≠ch n√†o. H√£y th·ª≠ ph√¢n t√≠ch b√†i vi·∫øt ƒë·∫ßu ti√™n!")

def analyze_callback(url):
    """Callback khi click ph√¢n t√≠ch b√†i vi·∫øt"""
    if not url or 'reddit.com' not in url:
        st.error("‚ö†Ô∏è URL kh√¥ng h·ª£p l·ªá. Vui l√≤ng ki·ªÉm tra l·∫°i.")
        return
    
    # Chu·∫©n h√≥a URL
    if not url.startswith('http'):
        url = 'https://' + url
    
    st.session_state.analyze_url = url
    st.session_state.auto_run = True
    st.session_state.page = "Analysis"
    st.rerun()

def trending_page():
    st.markdown("## üìä Ph√¢n T√≠ch Xu H∆∞·ªõng")
    user = st.session_state.user
    groups = db.get_groups(user['id'])
    
    if not groups:
        st.info("üí° Ch∆∞a c√≥ nh√≥m theo d√µi. H√£y th√™m nh√≥m ·ªü thanh b√™n tr√°i.")
        return
    
    # Kh·ªüi t·∫°o managers
    trend_manager = TrendAnalysisManager()
    trending_manager = TrendingManager()
    
    # Control panel
    st.markdown("### ‚öôÔ∏è C√†i ƒë·∫∑t ph√¢n t√≠ch")
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        analysis_days = st.selectbox(
            "Th·ªùi gian ph√¢n t√≠ch",
            options=[7, 14, 30],
            index=0,
            help="S·ªë ng√†y d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch"
        )
    with col2:
        posts_limit = st.slider("S·ªë b√†i vi·∫øt", 10, 50, 20, help="S·ªë b√†i vi·∫øt l·∫•y t·ª´ m·ªói subreddit")
    
    with col3:
        if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu", type="primary", use_container_width=True):
            with st.spinner("ƒêang thu th·∫≠p d·ªØ li·ªáu m·ªõi..."):
                subs = [g['subreddit'] for g in groups]
                st.session_state.trending_data = trending_manager.fetch_feed(subs, limit=posts_limit)
                st.session_state.last_update = datetime.now()
                st.success(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t {len(st.session_state.trending_data)} b√†i vi·∫øt")
                st.rerun()
    
    # Hi·ªÉn th·ªã th√¥ng tin c·∫≠p nh·∫≠t
    if st.session_state.get('last_update'):
        st.info(f"üìÖ C·∫≠p nh·∫≠t l·∫ßn cu·ªëi: {st.session_state.last_update.strftime('%H:%M %d/%m')} | " +
               f"üìù T·ªïng b√†i vi·∫øt: {len(st.session_state.get('trending_data', []))}")

    if not st.session_state.get('trending_data'):
        st.info("üëÜ Nh·∫•n 'C·∫≠p nh·∫≠t d·ªØ li·ªáu' ƒë·ªÉ b·∫Øt ƒë·∫ßu ph√¢n t√≠ch xu h∆∞·ªõng")
        return

    # L·ªçc subreddit
    all_subs = sorted(list(set([p['subreddit'] for p in st.session_state.trending_data])))
    
    if not all_subs:
        st.warning("Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ subreddits. Vui l√≤ng th·ª≠ l·∫°i.")
        return
    
    st.markdown("### üîç Ch·ªçn c·ªông ƒë·ªìng ph√¢n t√≠ch")
    selected_subs = st.multiselect(
        "Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu c·ªông ƒë·ªìng:",
        options=all_subs,
        default=all_subs[:min(3, len(all_subs))],
        placeholder="Ch·ªçn c·ªông ƒë·ªìng..."
    )
    
    if not selected_subs:
        st.info("üéØ H√£y ch·ªçn √≠t nh·∫•t m·ªôt c·ªông ƒë·ªìng ƒë·ªÉ ph√¢n t√≠ch")
        return

    # Tab layout cho multiple subreddits
    st.markdown("### üìà K·∫øt qu·∫£ ph√¢n t√≠ch")
    tabs = st.tabs([f"r/{sub}" for sub in selected_subs])
    
    for idx, sub in enumerate(selected_subs):
        with tabs[idx]:
            # L·ªçc d·ªØ li·ªáu cho subreddit hi·ªán t·∫°i
            sub_posts = [p for p in st.session_state.trending_data if p['subreddit'] == sub]
            
            if not sub_posts:
                st.warning(f"Kh√¥ng c√≥ d·ªØ li·ªáu cho r/{sub}")
                continue
            
            # Ph√¢n t√≠ch xu h∆∞·ªõng
            with st.spinner(f"ü§ñ ƒêang ph√¢n t√≠ch r/{sub} ({len(sub_posts)} b√†i vi·∫øt)..."):
                analysis_result = trend_manager.analyze_subreddit_trends(
                    subreddit=sub,
                    posts_data=sub_posts,
                    days=analysis_days
                )
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi ƒë·∫ßy ƒë·ªß ƒë·ªì th·ªã
            ui.render_trend_analysis(analysis_result)
            
            st.divider()
            
            # Hi·ªÉn th·ªã b√†i vi·∫øt t·ª´ subreddit n√†y
            st.markdown(f"### üìù B√†i vi·∫øt g·∫ßn ƒë√¢y t·ª´ r/{sub}")
            for post in sub_posts[:5]:
                ui.render_trending_card(post, analyze_callback)

def analysis_page():
    st.markdown("## üîó Ph√¢n T√≠ch B√†i Vi·∫øt")
    
    # URL input ƒë∆°n gi·∫£n - kh√¥ng hi·ªÉn th·ªã fallback options
    url = st.text_input(
        "URL Reddit:",
        value=st.session_state.get('analyze_url', ""),
        placeholder="https://www.reddit.com/r/...",
        help="D√°n link b√†i vi·∫øt Reddit b·∫•t k·ª≥"
    )
      
    # Ki·ªÉm tra URL c∆° b·∫£n
    url_valid = False
    if url:
        if 'reddit.com' not in url:
            st.warning("‚ö†Ô∏è URL kh√¥ng ph·∫£i l√† Reddit. Vui l√≤ng nh·∫≠p URL Reddit h·ª£p l·ªá.")
        elif not url.startswith('http'):
            st.warning("‚ö†Ô∏è URL ph·∫£i b·∫Øt ƒë·∫ßu v·ªõi http:// ho·∫∑c https://")
        else:
            url_valid = True
    
    auto_run = st.session_state.get('auto_run', False)
    
    col1, col2 = st.columns([1, 1])
    with col1:
        if st.button("üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch", type="primary", use_container_width=True) or (auto_run and url_valid):
            st.session_state.auto_run = False
            run_analysis(url)
    
    with col2:
        if st.button("üîÑ X√≥a k·∫øt qu·∫£", type="secondary", use_container_width=True):
            if 'analysis_result' in st.session_state:
                del st.session_state.analysis_result
            st.rerun()

def run_analysis(url):
    """Ch·∫°y ph√¢n t√≠ch b√†i vi·∫øt v·ªõi fallback t·ª± ƒë·ªông"""
    with st.status("üîÑ ƒêang ph√¢n t√≠ch...", expanded=True) as status:
        try:
            loader = RedditLoader()
            ai = AdvancedAIAnalyst()
            
            status.write("üì• **1. T·∫£i d·ªØ li·ªáu t·ª´ Reddit...**")
            data, err = loader.fetch_data(url)
            
            if err:
                # TH√îNG B√ÅO L·ªñI ƒê∆†N GI·∫¢N
                if "403" in err or "ch·∫∑n" in err:
                    st.error(f"""
                    üîí **Kh√¥ng th·ªÉ truy c·∫≠p b√†i vi·∫øt**
                    
                    Reddit ƒëang ch·∫∑n truy c·∫≠p t·ª´ server n√†y.
                    H·ªá th·ªëng ƒë√£ t·ª± ƒë·ªông th·ª≠ c√°c ph∆∞∆°ng th·ª©c thay th·∫ø nh∆∞ng kh√¥ng th√†nh c√¥ng.
                    
                    **ƒê·ªÅ xu·∫•t:**
                    1. Th·ª≠ l·∫°i sau 1-2 ph√∫t
                    2. Th·ª≠ b√†i vi·∫øt kh√°c
                    3. Ki·ªÉm tra xem b√†i vi·∫øt c√≥ t·ªìn t·∫°i kh√¥ng
                    """)
                else:
                    st.error(f"‚ùå L·ªói: {err}")
                
                status.update(state="error")
                return
            
            st.success(f"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng: {data['meta']['title'][:80]}...")
            st.info(f"Subreddit: r/{data['meta']['subreddit']} ‚Ä¢ üë§ {data['meta']['author']} ‚Ä¢ üëç {data['meta']['score']}")
            
            status.write("ü§ñ **2. X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n...**")
            df = process_nlp(data['comments']) if data['comments'] else pd.DataFrame()
            
            status.write("üß† **3. Ph√¢n t√≠ch AI v·ªõi Gemini 2.0 Flash...**")
            summary = ai.analyze(data['meta'], data['comments'])
            
            status.write("üíæ **4. L∆∞u l·ªãch s·ª≠...**")
            db.add_history(st.session_state.user['id'], data['meta']['title'], url)
            
            st.session_state.analysis_result = {
                'meta': data['meta'], 
                'df': df, 
                'summary': summary,
                'url': url,
                'analyzed_at': datetime.now()
            }
            
            status.update(state="complete", label="‚úÖ Ph√¢n t√≠ch ho√†n t·∫•t!")
            
        except Exception as e:
            st.error(f"‚ùå L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch: {str(e)}")
            status.update(state="error")

def display_analysis_results():
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ ph√¢n t√≠ch"""
    if not st.session_state.get('analysis_result'):
        return
    
    result = st.session_state.analysis_result
    
    # Tabs cho c√°c ph·∫ßn ph√¢n t√≠ch
    tab1, tab2, tab3, tab4 = st.tabs(["üìä T·ªïng quan", "üìà Bi·ªÉu ƒë·ªì", "üí¨ B√¨nh lu·∫≠n", "üì• Xu·∫•t d·ªØ li·ªáu"])
    
    with tab1:
        display_overview_tab(result)
    
    with tab2:
        display_charts_tab(result)
    
    with tab3:
        display_comments_tab(result)
    
    with tab4:
        display_export_tab(result)

def display_overview_tab(result):
    """Tab t·ªïng quan"""
    # Header v·ªõi th√¥ng tin b√†i vi·∫øt
    st.markdown(f"## üìÑ {result['meta']['title']}")
    
    # Th√¥ng tin c∆° b·∫£n
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Subreddit", f"r/{result['meta']['subreddit']}")
    with col2:
        st.metric("ƒêi·ªÉm", result['meta']['score'])
    with col3:
        st.metric("B√¨nh lu·∫≠n", len(result['df']) if not result['df'].empty else 0)
    with col4:
        ratio = result['meta'].get('upvote_ratio', 0)
        st.metric("Upvote Ratio", f"{ratio:.1%}")
    
    # Th·ªùi gian v√† t√°c gi·∫£
    col_info1, col_info2 = st.columns(2)
    with col_info1:
        st.metric("‚úçÔ∏è T√°c gi·∫£", result['meta']['author'])
    with col_info2:
        st.metric("üïê ƒêƒÉng b√†i", result['meta'].get('created_time', 'Kh√¥ng r√µ'))
    
    # AI Insight
    st.markdown("### ü§ñ Ph√¢n T√≠ch AI Chi Ti·∫øt")
    st.markdown("---")
    
    if result['summary']:
        st.markdown(result['summary'])
    else:
        st.warning("Kh√¥ng c√≥ ph√¢n t√≠ch AI")

def display_charts_tab(result):
    """Tab bi·ªÉu ƒë·ªì"""
    # Bi·ªÉu ƒë·ªì c·∫£m x√∫c
    st.markdown("### üìä Bi·ªÉu ƒê·ªì Ph√¢n T√≠ch C·∫£m X√∫c")
    
    if not result['df'].empty:
        # T·∫°o visualizations
        try:
            fig1, fig2, fig3 = create_visualization(result['df'])
            
            # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì
            col_chart1, col_chart2 = st.columns(2)
            
            with col_chart1:
                st.plotly_chart(fig1, use_container_width=True)
            
            with col_chart2:
                st.plotly_chart(fig2, use_container_width=True)
            
            # Scatter plot
            st.plotly_chart(fig3, use_container_width=True)
            
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o bi·ªÉu ƒë·ªì: {e}")
            
            # Fallback: hi·ªÉn th·ªã bar chart ƒë∆°n gi·∫£n
            sentiment_counts = result['df']['sentiment'].value_counts()
            if not sentiment_counts.empty:
                st.bar_chart(sentiment_counts)
    else:
        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu b√¨nh lu·∫≠n ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì")

def display_comments_tab(result):
    """Tab b√¨nh lu·∫≠n"""
    # B√¨nh lu·∫≠n chi ti·∫øt
    st.markdown("### üí¨ Ph√¢n T√≠ch B√¨nh Lu·∫≠n Chi Ti·∫øt")
    
    if not result['df'].empty:
        # Filter v√† s·∫Øp x·∫øp
        st.markdown("#### üîç L·ªçc b√¨nh lu·∫≠n")
        col_filter1, col_filter2 = st.columns(2)
        
        with col_filter1:
            sentiment_filter = st.multiselect(
                "L·ªçc theo c·∫£m x√∫c",
                options=result['df']['sentiment'].unique(),
                default=result['df']['sentiment'].unique()[:3]
            )
        
        with col_filter2:
            sort_by = st.selectbox(
                "S·∫Øp x·∫øp theo",
                options=['score', 'polarity', 'word_count'],
                index=0
            )
        
        # L·ªçc d·ªØ li·ªáu
        filtered_df = result['df']
        if sentiment_filter:
            filtered_df = filtered_df[filtered_df['sentiment'].isin(sentiment_filter)]
        
        # S·∫Øp x·∫øp
        filtered_df = filtered_df.sort_values(sort_by, ascending=False)
        
        # Hi·ªÉn th·ªã b√¨nh lu·∫≠n
        st.markdown(f"#### üìù B√¨nh lu·∫≠n ({len(filtered_df)}/{len(result['df'])})")
        
        for idx, row in filtered_df.head(20).iterrows():
            with st.container():
                col_comment1, col_comment2 = st.columns([4, 1])
                
                with col_comment1:
                    st.markdown(f"**{row['sentiment_emoji']} {row['sentiment']}** ‚Ä¢ üë§ {row['author']}")
                    st.markdown(f"> {row['text']}")
                
                with col_comment2:
                    st.metric("ƒêi·ªÉm", row['score'])
                    st.caption(f"Polarity: {row['polarity']:.3f}")
                
                st.divider()
        
        # Th·ªëng k√™
        st.markdown("#### üìà Th·ªëng k√™ b√¨nh lu·∫≠n")
        col_stats1, col_stats2, col_stats3 = st.columns(3)
        
        with col_stats1:
            avg_polarity = filtered_df['polarity'].mean()
            st.metric("üé≠ ƒê·ªô c·ª±c t√≠nh TB", f"{avg_polarity:.3f}")
        
        with col_stats2:
            avg_score = filtered_df['score'].mean()
            st.metric("‚≠ê ƒêi·ªÉm TB", f"{avg_score:.1f}")
        
        with col_stats3:
            total_words = filtered_df['word_count'].sum()
            st.metric("üìù T·ªïng s·ªë t·ª´", total_words)
        
    else:
        st.info("Kh√¥ng c√≥ b√¨nh lu·∫≠n ƒë·ªÉ ph√¢n t√≠ch")

def display_export_tab(result):
    """Tab xu·∫•t d·ªØ li·ªáu"""
    # Xu·∫•t d·ªØ li·ªáu
    st.markdown("### üì• Xu·∫•t D·ªØ Li·ªáu Ph√¢n T√≠ch")
    
    if not result['df'].empty:
        # T·∫°o DataFrame cho export
        export_df = result['df'].copy()
        
        # Th√™m th√¥ng tin b√†i vi·∫øt
        export_df['post_title'] = result['meta']['title']
        export_df['post_subreddit'] = result['meta']['subreddit']
        export_df['post_score'] = result['meta']['score']
        export_df['post_author'] = result['meta']['author']
        export_df['analysis_date'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Hi·ªÉn th·ªã preview
        st.markdown("#### üëÅÔ∏è Preview d·ªØ li·ªáu")
        st.dataframe(export_df.head(10), use_container_width=True)
        
        # Download options
        st.markdown("#### üíæ T·∫£i xu·ªëng")
        
        col_dl1, col_dl2, col_dl3 = st.columns(3)
        
        with col_dl1:
            # CSV
            csv = export_df.to_csv(index=False)
            st.download_button(
                label="üì• T·∫£i CSV",
                data=csv,
                file_name=f"reddit_analysis_{result['meta']['subreddit']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        with col_dl2:
            # Excel
            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                export_df.to_excel(writer, index=False, sheet_name='Sentiment_Analysis')
            
            st.download_button(
                label="üìä T·∫£i Excel",
                data=excel_buffer.getvalue(),
                file_name=f"reddit_analysis_{result['meta']['subreddit']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
        
        with col_dl3:
            # JSON
            json_data = export_df.to_json(orient='records', indent=2)
            st.download_button(
                label="üìÑ T·∫£i JSON",
                data=json_data,
                file_name=f"reddit_analysis_{result['meta']['subreddit']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True
            )
        
        # Th·ªëng k√™ export
        st.markdown("---")
        col_info1, col_info2 = st.columns(2)
        with col_info1:
            st.metric("üìä S·ªë d√≤ng d·ªØ li·ªáu", len(export_df))
        with col_info2:
            st.metric("üìà S·ªë c·ªôt d·ªØ li·ªáu", len(export_df.columns))
        
    else:
        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ xu·∫•t")
    
    # Link v·ªÅ b√†i vi·∫øt g·ªëc
    st.markdown("---")
    st.markdown(f"**üîó Link b√†i vi·∫øt g·ªëc:** [{result['meta']['title'][:50]}...]({result.get('url', '#')})")

# --- MAIN ---
def main():
    # 1. Kh·ªüi t·∫°o Session State
    default_state = {
        'authenticated': False,
        'user': None,
        'page': "Dashboard",
        'trending_data': [],
        'analyze_url': "",
        'auto_run': False,
        'analysis_result': None,
        'last_update': None
    }
    
    for key, value in default_state.items():
        if key not in st.session_state:
            st.session_state[key] = value

    # 2. Load UI
    ui.load_css()
    
    # 3. Routing
    if not st.session_state.authenticated:
        login_page()
    else:
        user = st.session_state.user
        groups = db.get_groups(user['id'])
        
        # Render sidebar
        ui.render_sidebar_logged_in(
            user['username'], 
            groups,
            lambda: (setattr(st.session_state, 'authenticated', False), st.rerun()), 
            lambda sub: (db.add_group(user['id'], sub), st.rerun()), 
            lambda gid: (db.delete_group(gid), st.rerun())
        )
        
        # Page routing
        if st.session_state.page == "Dashboard":
            dashboard_page()
        elif st.session_state.page == "Trending":
            trending_page()
        elif st.session_state.page == "Analysis":
            analysis_page()
            display_analysis_results()

# Ch·∫°y ·ª©ng d·ª•ng
if __name__ == "__main__":
    main()